{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Credit Card Fraud Detection\n",
    "\n",
    "Familiarizing myself with methods to handle unbalanced data with Python then build a binary classification model to predict credit card frauds. It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase, and I actually had some fraud purchases happen not too long ago on myself as well but it was not flagged by the bank.\n",
    "\n",
    "Goal here is to review methods in the preprocessing/cleaning data, unbalanced dataset solutions for modleing, and review the classification process.\n",
    "\n",
    "*_Note: Codes may also come from other sources for practice/workflow. Informational links have been added at the bottom of the notebook._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else do I hope to achieve from this?\n",
    "\n",
    "Develop a stronger understanding in data cleaning, understand why data is missing, how to work with unbalanced data which is normal in real world situations, and review methods on classification methods to combat this issue. I also want to utilize PCA, feature selection, and sklearn's pipeline function more often so I will attempt to use it on this dataset as well.\n",
    "\n",
    "I also follow the same methodology and attempt to improve on it as much as I can on each project. See link [here](https://github.com/davidtnly/Python/tree/master/MachineLearning) for public notebooks.\n",
    "\n",
    "Answer some of these questions below:\n",
    "- What kind of machine learning algorithms are good against unbalanced datasets?\n",
    "- What are the risks in our modeling and evaluation methods?\n",
    "- Is my modeling process efficient with the use of pipeline() and is the EDA process useful? \n",
    "- What's SMOTE, Undersampling and Oversampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing the problem\n",
    "\n",
    "Can you identify fraudulent credit card transactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data comes from the [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset on Kaggle.\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project.\n",
    "\n",
    "Please take a look at the Works Cited below for any additional information. They are all informative and gives you another perspective in working with machine learning and deep learning in the financial field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Framework\n",
    "\n",
    "As always, here's a ML framework I like to follow or reference back to whenever I work on a project.\n",
    "\n",
    "1. Framing the problem\n",
    "    - What are we trying to solve?\n",
    "    - Understand what's the problem here and ask questions\n",
    "    - What type of problem are we trying to solve?\n",
    "        - Classification, regression, etc.\n",
    "        - Types of algorithms specified for possible use\n",
    "2. Collecting relevant information and data\n",
    "    - What type of data do we have?\n",
    "    - Can we use any complimentary data with that is public that will help our with our analysis/modeling?\n",
    "    - What other data requirements are there?\n",
    "    - What is considered a success for this problem?\n",
    "    - What libraries do we need?\n",
    "        - How do we know if our models are good?\n",
    "3. Process for analysis (preprocessing & cleaning)\n",
    "    - How does the data structure and distribution look like?\n",
    "    - Is the data usable right away?\n",
    "    - Can the data be plotted?\n",
    "    - What changes do we need in order to make the data usable if it's not already?\n",
    "    - This step is not really an isolated step as it can encompass exploratory, feature engineering + more\n",
    "4. Exploratory data analysis\n",
    "    - How does the data look like?\n",
    "    - Are there any patterns?\n",
    "        - Identify any summary statistics, plotting, counting, etc.\n",
    "    - Familiarize yourself with the data\n",
    "    - Basically the step to help you get to know the data better\n",
    "5. Feature engineering (applied machine learning)\n",
    "    - Can we create more data (features) that will be helpful for our models?\n",
    "    - \"... re-working of predictors is more of an art, requiring the right tools and experience to find better predictor representations\" - Max Kuhn\n",
    "6. Statistical analysis\n",
    "    - Univariate, bivariate, multivariate analysis\n",
    "        - Analysis of a single feature\n",
    "        - Analysis of two features and their relationships\n",
    "        - Analysis of data collected on more than one dependent variable and their relationships (PCA, PLS)\n",
    "7. Model development & scoring\n",
    "    - Splitting the data into train and test sets\n",
    "        - Always make sure you have a completely separate data to test your final model on after hyperparameter tuning and training has been done\n",
    "        - Think about your experimental design beforehand so that you minimize unrelated sources of variation and reduce as much data leakage (if any) as possible\n",
    "    - Normalizing the data to be on similar scales\n",
    "        - Normalization, standardization methods (Z-score, minmax, median)\n",
    "    - Create baseline, pre-tuned, and tuned models (includes cross-validation)\n",
    "        - Create easy to more complex models if needed (think about computational cost, complexity, explainability)\n",
    "    - Hyperparameter tuning\n",
    "    - Fit cross-validated tuned models using best hyperparameters\n",
    "    - Score model and get results\n",
    "8. Evaluation\n",
    "    - How accurate are the models?\n",
    "        - Are the models overfitting or underfitting?\n",
    "    - What evaluation metric are we using?\n",
    "    - Is the final model good enough?\n",
    "    - Which features are important?\n",
    "    - Iterate steps if we are proceeding with specific features selected\n",
    "        - Dimensionality reduction methods (PCA, LDA) to see if we can reduce model complexity (this step can be before modeling as well)\n",
    "9. Results\n",
    "    - What's our conclusion?\n",
    "    - What actions are we going to take?\n",
    "10. Ending notes\n",
    "    - Extra information that we may not go over like steps that we did not go into or missed\n",
    "        - Any other possible methods/solutions that we could look at in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Evaluation\n",
    "\n",
    "Since this is a binary classification problem, we will start our baseline model using a logistic regression algorithm. \n",
    "\n",
    "The evaluation method will be the AUPRC score which is the recommended method. AUPRC is the area under the curve where X is recall and Y is precision. This is a better measure to seek a balance between precision and recall vs. accuracy especially when there there is a higher risk in misclassifying a purchase as the bank's risk is a lot higher when it is not classified correctly or on time. Precision-Recall curves also should be used when there is a moderate to large class imbalance\n",
    "\n",
    "Here are some performance metrics used in the confusion matrix below:\n",
    "\n",
    "- TP - correct positive prediction\n",
    "- TN - correct negative prediction\n",
    "- FP - Type I Error (case negative but predicted positive)\n",
    "- FN - Type II Error (case positive but predicted negative)\n",
    "- Precision - how accurate is the model of those predicted positive that are actually positive\n",
    "    - TP/(TP + FP)\n",
    "- Recall / Sensitivity / TPR (True Positive Rate) - how many actual positives the model labeled over the total positives\n",
    "    - TP/(TP + FN) which is also TP/All P\n",
    "- Specificity (True Negative Rate) - number of correct negative predictions\n",
    "    - TN/(TN + FP) which is also TN/All N\n",
    "- F1 Score - weighted average of TPR (Recall) and Precision, useful if there is uneven class distribution like cancer detection where there can be 10,000 negatives and only 1 positive or if there is a higher risk if false negatives or false positives\n",
    "    - 2 * ( (Pr * Rc) / (Pr + Rc) )\n",
    "- ROC Curve\n",
    "    - Graph used to summarize the performance at various thresholds by plotting the TPR against FPR\n",
    "    \n",
    "    \n",
    "#### Methods\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import auc\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    f1 = f1_score(y_true, yhat)\n",
    "    auc = auc(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Modeling Process\n",
    "\n",
    "In order to see if fraud is detected or not, we will use multiple classification algorithms to predict the outcome of 0 (negative) or 1 (positive). The employed classifiers are logistic regression, decision trees, random forest, support vector machines, and extreme gradient boosting. \n",
    "\n",
    "After each performance of the machine learning method, we will begin evaluation where we will use 10-fold cross-validation as well, which is simply a procedure used to evaluate ML models where data is limited. In the cross-validation, data is resampled to best test our evaluation metric then averaged to get an overall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABECAYAAAAiJuZQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAABr0lEQVR4nO3YvUkEURiF4VkRNhJx1cREEGTRRLABKzAx08AmLMEeBCuwAquwgsFANl4RDAQR5NrA7kTec/15nvRLzsDwBndUSukAyFhpPQDgPxFdgCDRBQgSXYAg0QUIWh06Hh0el/XxJLUlbr42bj2hqq2Vt9YTqtl4mbeeUNVj97f/zbIzmJ5frzy9Pvd9v73oNvjl6+NJd3lwVWfVD3B7st96QlXnaw+tJ1RzdnfTekJVp597rSdU9XG92XpCVe8X97NlN88LAEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaNSytLjdDqdd103y80B+BN2+77fXnQYjC4A38vzAkCQ6AIEiS5AkOgCBIkuQNAXNFUnREA1iFAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Toolbox\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import norm, skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "from scipy import stats\n",
    "\n",
    "# Misc\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Preset data display\n",
    "pd.options.display.max_seq_items = 1000\n",
    "pd.options.display.max_rows =1000\n",
    "\n",
    "# Set notebook colors and palette\n",
    "flatui = ['#9b59b6', '#3498db', '#95a5a6', '#e74c3c', '#34495e', '#2ecc71']\n",
    "sns.set_palette(flatui)\n",
    "sns.palplot(sns.color_palette(flatui))\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes(palette='deep')\n",
    "# Favorite code to use: #34995e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19-credit-card-defaults.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set directory\n",
    "PATH = 'C:\\\\Users\\\\' + os.getlogin() + '\\\\Documents\\\\Programming\\\\Python\\\\MachineLearning\\\\LargeData'\n",
    "os.chdir(PATH)\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('19-credit-card-defaults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Let's take a look at the data and see the structure of the data. It's good to know how big our dataset is (200k+) so we are careful in using certain functions or plots and making sure we keep in mind our memory usage. Besides that, we will also be looking for missing values as well to get an idea on how complete the data is. We can create summary and diagnostic plots to show this. The less missing values the better and then clean up missing values and try to understand why are they missing in the first place.\n",
    "\n",
    "In summary, some of the questions we try to answer here are:\n",
    "- Are we able to use the data right away?\n",
    "    - If not, why?\n",
    "- Are there any missing values?\n",
    "    - If not, what methods should we use to fix this?\n",
    "- How clean is the data?\n",
    "    - Are all values the same in a single feature?\n",
    "    - Is the feature format consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Some basic functions:\n",
    "\n",
    "train.sample()                                           \n",
    "train.describe()\n",
    "    train.describe(include=['O'])\n",
    "    train.describe(include='all')\n",
    "train.head()\n",
    "train.tail()\n",
    "train.value_counts().sum()\n",
    "train.isnull().sum()\n",
    "train.count()\n",
    "train.fillna()\n",
    "    train.fillna(train[col].mode(), inplace=True)\n",
    "train.mean()\n",
    "train.median()\n",
    "train.mode()\n",
    "train.shape\n",
    "train.info()\n",
    "\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration. \n",
    "\n",
    "You should have pandas functions like .corr(), scatter_matrix() , .hist() and .bar() on the tip of your tongue. You should always be looking for opportunities to visualize your data using PCA or t-SNE, using sklearn's PCA and TSNE functions.\n",
    "\n",
    "Feature selection. \n",
    "\n",
    "90% of the time, your dataset will have way more features than you need (which leads to excessive training time, and a heightened risk of overfitting). Get familiar with basic filter methods (look up scikit-learn’s VarianceThreshold and SelectKBest functions), and more sophisticated model-based feature selection methods (look up SelectFromModel).\n",
    "\n",
    "Hyperparameter search for model optimization. \n",
    "\n",
    "You definitely should know what GridSearchCV does and how it works. Likewise for RandomSearchCV. To really stand out, try experimenting with skopt's BayesSearchCV to learn how you can apply bayesian optimization to your hyperparameter search.\n",
    "\n",
    "Pipelines. \n",
    "\n",
    "Use sklearn's pipeline library to wrap their preprocessing, feature selection and modeling steps together. Discomfort with pipeline is a huge tell that a data scientist needs to get more familiar with their modeling toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works Cited\n",
    "\n",
    "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. [Calibrating Probability with Undersampling for Unbalanced Classification](https://www.researchgate.net/publication/283349138_Calibrating_Probability_with_Undersampling_for_Unbalanced_Classification). In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. [Learned lessons in credit card fraud detection from a practitioner perspective](https://www.researchgate.net/publication/260837261_Learned_lessons_in_credit_card_fraud_detection_from_a_practitioner_perspective), Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "\n",
    "Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. [Credit card fraud detection: a realistic modeling and a novel learning strategy](https://www.researchgate.net/publication/319867396_Credit_Card_Fraud_Detection_A_Realistic_Modeling_and_a_Novel_Learning_Strategy), IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "\n",
    "Dal Pozzolo, Andrea. [Adaptive Machine learning for credit card fraud detection](http://di.ulb.ac.be/map/adalpozz/pdf/Dalpozzolo2015PhD.pdf) ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "\n",
    "Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. [Scarff: a scalable framework for streaming credit card fraud detection with Spark](https://www.researchgate.net/publication/319616537_SCARFF_a_Scalable_Framework_for_Streaming_Credit_Card_Fraud_Detection_with_Spark), Information fusion,41, 182-194,2018,Elsevier\n",
    "\n",
    "Carcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. [Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization](https://www.researchgate.net/publication/332180999_Deep-Learning_Domain_Adaptation_Techniques_for_Credit_Cards_Fraud_Detection), International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n",
    "\n",
    "Bertrand Lebichot, Yann-Aël Le Borgne, Liyun He, Frederic Oblé, Gianluca Bontempi. [Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection](https://www.researchgate.net/publication/332180999_Deep-Learning_Domain_Adaptation_Techniques_for_Credit_Cards_Fraud_Detection), INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n",
    "\n",
    "Fabrizio Carcillo, Yann-Aël Le Borgne, Olivier Caelen, Frederic Oblé, Gianluca Bontempi. [Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection](https://www.researchgate.net/publication/333143698_Combining_Unsupervised_and_Supervised_Learning_in_Credit_Card_Fraud_Detection), Information Sciences, 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
