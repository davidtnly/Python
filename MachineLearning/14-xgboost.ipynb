{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regularization:\n",
    "    - Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.\n",
    "    - XGBoost is also known as ‘regularized boosting‘ technique.\n",
    "2. Parallel Processing:\n",
    "    - XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
    "    - Boosting is sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from. Check this link out to explore further.\n",
    "3. High Flexibility\n",
    "    - XGBoost allow users to define custom optimization objectives and evaluation criteria.\n",
    "    - This adds a whole new dimension to the model and there is no limit to what we can do.\n",
    "4. Handling Missing Values\n",
    "    - XGBoost has an in-built routine to handle missing values.\n",
    "    - User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.\n",
    "5. Tree Pruning:\n",
    "    - A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.\n",
    "    - XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
    "6. Built-in Cross-Validation\n",
    "    - XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "    - This is unlike GBM where we have to run a grid-search and only a limited values can be tested.\n",
    "7. Continue on Existing Model\n",
    "    - User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM:\n",
      "[[104  23]\n",
      " [ 29  36]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       127\n",
      "           1       0.61      0.55      0.58        65\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       192\n",
      "   macro avg       0.70      0.69      0.69       192\n",
      "weighted avg       0.72      0.73      0.73       192\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy Score: 0.7292\n",
      "Precision: 0.6101694915254238\n",
      "Recall: 0.5538461538461539\n",
      "F1 Score: 0.5806451612903227\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import metrics that would allow us to see how accurate the predictions are\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Data/11-diabetes.csv')\n",
    "\n",
    "# Split data into X and y\n",
    "X = data.iloc[:, 0:8]\n",
    "y = data.iloc[:,8]\n",
    "\n",
    "# Split data into train and test sets\n",
    "seed = 100\n",
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# Fit model\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate\n",
    "print('CM:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Evaluation Metrics:')\n",
    "print('Accuracy Score: {}'.format(round(accuracy_score(y_test, y_pred), 4)))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM:\n",
      "[[106  23]\n",
      " [ 26  37]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       129\n",
      "           1       0.62      0.59      0.60        63\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       192\n",
      "   macro avg       0.71      0.70      0.71       192\n",
      "weighted avg       0.74      0.74      0.74       192\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy Score: 0.7448\n",
      "Precision: 0.6166666666666667\n",
      "Recall: 0.5873015873015873\n",
      "F1 Score: 0.6016260162601625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler = RobustScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Split data into train and test sets\n",
    "seed = 150\n",
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# Transform the variables to be on the same scale\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit model\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate\n",
    "print('CM:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Evaluation Metrics:')\n",
    "print('Accuracy Score: {}'.format(round(accuracy_score(y_test, y_pred), 4)))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
