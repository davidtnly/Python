{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Predict North America video game sales with a dataset from [Kaggle](https://www.kaggle.com/gregorut/videogamesales).\n",
    "\n",
    "Personal goal is to use Python for data cleaning and classical machine learning, so there will be a good amount of notes per method used.\n",
    "\n",
    "*Note: Codes may also come from other sources for practice/workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else do I hope to achieve from this?\n",
    "\n",
    "Develop a stronger understanding in experimental design before developing prediction models. It's the idea of the laying out of a detailed experimental plan in advance of doing the experiment.\n",
    "- How should we best represent splitting the data?\n",
    "- How do we control the environment to best mimic a real-world study?\n",
    "\n",
    "Develop a stronger understanding using Python functions and methods in the entire modeling lifecycle after data preparation. \n",
    "- What's best practice? \n",
    "- Should I create a bunch of wrapper functions to automate everything? \n",
    "- Should I start building in a granular form first? \n",
    "- Do I understand the algorithms being implemented?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This dataset contains a list of video games with sales greater than 100,000 copies. It was generated by a scrape of vgchartz.com.\n",
    "\n",
    "Note: This could be another project where I could modify the scraper for practice.\n",
    "\n",
    "__Feature Descriptions:__\n",
    "\n",
    "1. Rank - Ranking of overall sales\n",
    "\n",
    "1. Name - The games name\n",
    "\n",
    "1. Platform - Platform of the games release\n",
    "\n",
    "1. Year - Year of the game's release\n",
    "\n",
    "1. Genre - Genre of the game\n",
    "\n",
    "1. Publisher - Publisher of the game\n",
    "\n",
    "1. NA_Sales - Sales in North America (in millions)\n",
    "\n",
    "1. EU_Sales - Sales in Europe (in millions)\n",
    "\n",
    "1. JP_Sales - Sales in Japan (in millions)\n",
    "\n",
    "1. Other_Sales - Sales in the rest of the world (in millions)\n",
    "\n",
    "1. Global_Sales - Total worldwide sales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing the problem\n",
    "\n",
    "Can you predict North America video game sales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Evaluation\n",
    "\n",
    "Since we are predicting a price, this will be a regression problem. We will use linear algorithms like linear regression, ridge regression, lasso regression, elastic-net regression and non-linear algorithms like random forest, gradient boosting, and a stacking method.\n",
    "\n",
    "The evaluation metric we will be using is __Root-Mean-Squared_error (RMSE)__ between actual sales and the predicted sales. RMSE is a quadratic scoring rule that also measures the average magnitude of the error. Itâ€™s the square root of the average of squared differences between prediction and actual observation.\n",
    "\n",
    "__Why RMSE over mean-squared-error (MAE)?__ Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5. But if being off by 10 is just twice as bad as being off by 5, then MAE is more appropriate.\n",
    "\n",
    "Here's a picture of the RMSE formula below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "image/png": {
       "height": 350,
       "width": 300
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "url = 'https://gisgeography.com/wp-content/uploads/2014/07/rmse-formula1-300x96.png'\n",
    "Image(url, width=300, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Framework\n",
    "\n",
    "1. Framing the problem\n",
    "    - What are we trying to solve?\n",
    "    - Understand what's the problem here and ask questions\n",
    "    - What type of problem are we trying to solve?\n",
    "        - Classification, regression, etc.\n",
    "        - Types of algorithms specified for possible use\n",
    "2. Collecting relevant information and data\n",
    "    - What type of data do we have?\n",
    "        - Databases, scraped data, API data, etc.\n",
    "    - What other data requirements are there?\n",
    "    - What is considered a success for this problem?\n",
    "    - What libraries do we need?\n",
    "        - How do we know our models are good?\n",
    "3. Process for analysis (preprocessing & cleaning)\n",
    "    - How does the data structure and distribution look like?\n",
    "    - Is the data usable right away? \n",
    "    - Can the data be plotted?\n",
    "    - What changes do we need to make the data usable if it's not already?\n",
    "4. Exploratory data analysis\n",
    "    - How does the data look like?\n",
    "    - Are there any patterns?\n",
    "        - Identify any summary statistics, plotting, counting, etc.\n",
    "    - Familiarize yourself with the data\n",
    "    - Basically the step to help you get to know the data better\n",
    "5. Feature engineering (applied machine learning)\n",
    "    - Can we create more data (features) that will be helpful for our models?\n",
    "    - \"... re-working of predictors is more of an art, requiring the right tools and experience to find better predictor representations\" - Max Kuhn\n",
    "6. Statistical analysis\n",
    "    - Univariate, bivariate, multivariate analysis\n",
    "        - Analysis of a single feature\n",
    "        - Analysis of two features and their relationships\n",
    "        - Analysis of data collected on more than one dependent variable and their relationships (PCA, PLS)\n",
    "7. Modeling & Scoring\n",
    "    - Splitting the data into train and test sets\n",
    "        - Always make sure you have a completely separate data set to test your final model on after hyperparameter tuning and training has been done\n",
    "        - Think about your experimental design beforehand so that you minimize unrelated sources of variation and reduce as much data leakage (if any) as possible\n",
    "    - Standardizing the data to be on similar scales\n",
    "    - Create baseline & pre-tuned models\n",
    "    - Cross-validate the models\n",
    "    - Hyperparameter tuning\n",
    "    - Create cross-validated tuned models using best hyperparameters\n",
    "    - Compare models\n",
    "8. Evaluation\n",
    "    - How accurate are the models?\n",
    "        - Are the models overfitting or underfitting?\n",
    "    - What evaluation metric are we using?\n",
    "    - Is the final model good enough?\n",
    "    - Which features are important?\n",
    "    - Iterate steps with feature selection\n",
    "9. Extra\n",
    "    - Create train vs. cross-validating learning curves per size\n",
    "10. Ending Notes\n",
    "    - Notes on algorithms used, models created, ways to improve performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01-ign.csv',\n",
       " '02-winequality-red.csv',\n",
       " '02-winequality-white.csv',\n",
       " '03-thanksgiving-2015-poll-data.csv',\n",
       " '05-ibm-sales-loss.csv',\n",
       " '07-test.csv',\n",
       " '07-train.csv',\n",
       " '09-house-regression-env.db',\n",
       " '09-house-test.csv',\n",
       " '09-house-train.csv',\n",
       " '10-vgsales.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set directory\n",
    "import os\n",
    "path = 'C:\\\\Users\\\\' + os.getlogin() + '\\\\Documents\\\\Programming\\\\Python\\\\MachineLearning\\\\Data'\n",
    "\n",
    "# Move to directory with the data\n",
    "os.chdir(path)\n",
    "\n",
    "# Check\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAABICAYAAADI6S+jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAACEklEQVR4nO3aMWpUYRSG4XMdISkUhSQgKChCOjunt7SycQc2swo7FyBYCLoBF6M2Iti4AhPsRBDk2NgY1DAwv9/M9Xm6udziO81bXGbq7gLg37uQHgDwvxJggBABBggRYIAQAQYIEWCAkIvnvTBN06qqVlVVe4v9u9eu3Bg+KuXk0n56wlCHiy/pCcNc/XySnjDUx95LTxiqry/SE4b6+uHTaXcfnX0+rfM/4FsHx/34/rONDtsmL+4dpycM9ejy6/SEYR6+ep6eMNSD77fTE4b69uQgPWGod8unb7t7efa5TxAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAydfffX5imVVWtfv68U1XvR48KOqyq0/SIQeZ8W5X7dt3c77vZ3UdnH54b4F9enqY33b3c6KwtMuf75nxblft23dzv+xOfIABCBBggZN0AvxyyYnvM+b4531blvl039/t+a61vwABsjk8QACECDBAiwAAhAgwQIsAAIT8AsnxTG0hUjfsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Toolbox\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy import stats\n",
    "\n",
    "# Misc\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Preset data display\n",
    "pd.options.display.max_seq_items = 5000\n",
    "pd.options.display.max_rows = 5000\n",
    "\n",
    "# Set notebook palette\n",
    "flatui = ['#9b59b6', '#3498db', '#95a5a6', '#e74c3c', '#34495e', '#2ecc71']\n",
    "sns.set_palette(flatui)\n",
    "sns.palplot(sns.color_palette(flatui))\n",
    "# Favorite color for use: #34995e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('10-vgsales.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess & Clean\n",
    "\n",
    "Let's take a look at the data and see how it's structured. We will also be lookin for missing values as well to get an idea how complete the data is and create some summary diagnostics/plots. The less missing values the better and if they are missing, why are they missing?\n",
    "\n",
    "In summary, some of the questions we try to answer here:\n",
    "- Are we able to use the data right away?\n",
    "- Are there any nul values?\n",
    "- How clean is the data?\n",
    "\n",
    "Here are some basic functions we can use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Some functions to start off with:\n",
    "\n",
    "train.sample()                                           \n",
    "train.describe()\n",
    "    train.describe(include=['O'])\n",
    "    train.describe(include='all')\n",
    "train.head()\n",
    "train.tail()\n",
    "train.value_counts().sum()\n",
    "train.isnull().sum()\n",
    "train.count()\n",
    "train.fillna()\n",
    "    train.fillna(train[col].mode(), inplace=True)\n",
    "train.mean()\n",
    "train.median()\n",
    "train.mode()\n",
    "train.shape\n",
    "train.info()\n",
    "\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Wii Sports</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.49</td>\n",
       "      <td>29.02</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.46</td>\n",
       "      <td>82.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Super Mario Bros.</td>\n",
       "      <td>NES</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>29.08</td>\n",
       "      <td>3.58</td>\n",
       "      <td>6.81</td>\n",
       "      <td>0.77</td>\n",
       "      <td>40.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mario Kart Wii</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.85</td>\n",
       "      <td>12.88</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.31</td>\n",
       "      <td>35.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Wii Sports Resort</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.75</td>\n",
       "      <td>11.01</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.96</td>\n",
       "      <td>33.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pokemon Red/Pokemon Blue</td>\n",
       "      <td>GB</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Role-Playing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.27</td>\n",
       "      <td>8.89</td>\n",
       "      <td>10.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>31.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      Name Platform    Year         Genre Publisher  \\\n",
       "0     1                Wii Sports      Wii  2006.0        Sports  Nintendo   \n",
       "1     2         Super Mario Bros.      NES  1985.0      Platform  Nintendo   \n",
       "2     3            Mario Kart Wii      Wii  2008.0        Racing  Nintendo   \n",
       "3     4         Wii Sports Resort      Wii  2009.0        Sports  Nintendo   \n",
       "4     5  Pokemon Red/Pokemon Blue       GB  1996.0  Role-Playing  Nintendo   \n",
       "\n",
       "   NA_Sales  EU_Sales  JP_Sales  Other_Sales  Global_Sales  \n",
       "0     41.49     29.02      3.77         8.46         82.74  \n",
       "1     29.08      3.58      6.81         0.77         40.24  \n",
       "2     15.85     12.88      3.79         3.31         35.82  \n",
       "3     15.75     11.01      3.28         2.96         33.00  \n",
       "4     11.27      8.89     10.22         1.00         31.37  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16598, 11)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goal of looking at this dataset is to predict video game sales, we need to have at least 1 train set and 1 test set so we will split the data here. We are splitting the data at the very beginning, because we attempt to remove the possibility of data leakage as much as we can when developing predictive models is the goal here.\n",
    "\n",
    "__Data leakage__ - when information from outside the training dataset is used to create the model\n",
    "\n",
    "Data leakage can cause you to create overly optimistic if not completely invalid predictive models because additional information can allow the model to learn or know something that it otherwise would not know, which will in turn invalidate the estimated performance of the model being constructed. - [MLM Link](https://machinelearningmastery.com/data-leakage-machine-learning/)\n",
    "\n",
    "In other words, you know information of your test beforehand and then attempt to train (study & learn) on that same test and then validate your train results (time studying and what you have learned) on a \"new\" test data (the same dataset you learned from). So did you really learn something?\n",
    "\n",
    "Techniques to minimize data leakage when building models\n",
    "1. Perform data preparation within your cross-validation folds\n",
    "    - One solution for this, we can re-prep the data within your cross validation folds including tasks like feature selection, outlier removal, encoding, feature scaling and projection methods for dimensionality reduction, and more\n",
    "2. Hold back a validation dataset for final sanity check of your developed models\n",
    "    - This approach splits your training dataset into 2 sets (1 train and 1 validation) and then save the validation dataset for use after the models have been created\n",
    "    - Once created, validate the performance on the validation set \n",
    "\n",
    "Common data preparation issues\n",
    "1. Normalizing or standardization your entire dataset and then estimating the performance of your model using cross-validation\n",
    "    - When this happens, the test data's information is being added into the training information so when we start testing, it's not a true representation of unknown data\n",
    "    - An example is when you attempt to rescale the data on the same distribution of data, which includes the test set, and then use those values as inputs in your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dependent and independent variables\n",
    "y = data['NA_Sales']\n",
    "X = data.drop(['NA_Sales'], axis=1)\n",
    "\n",
    "# Shuffle the data and then split the data into train and test sets\n",
    "# Method 1: Shuffling - most recommended method to save time; drawback includes list ordering is lost\n",
    "# Method 2: Sampling - creates a new shuffled list and returns it rather than disturbing the order of original list\n",
    "from sklearn.utils import shuffle\n",
    "shuffled_data = shuffle(data).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75% of the data size =  12448.0\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "print('75% of the data size = ', (round(len(data)*0.75, 0)))\n",
    "train = shuffled_data[0:12448]\n",
    "test = shuffled_data[12448:len(shuffled_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "********** train shape: (12448, 12)**********\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12448 entries, 0 to 12447\n",
      "Data columns (total 12 columns):\n",
      "index           12448 non-null int64\n",
      "Rank            12448 non-null int64\n",
      "Name            12448 non-null object\n",
      "Platform        12448 non-null object\n",
      "Year            12245 non-null float64\n",
      "Genre           12448 non-null object\n",
      "Publisher       12404 non-null object\n",
      "NA_Sales        12448 non-null float64\n",
      "EU_Sales        12448 non-null float64\n",
      "JP_Sales        12448 non-null float64\n",
      "Other_Sales     12448 non-null float64\n",
      "Global_Sales    12448 non-null float64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "*******************************************\n",
      "********** test shape: (4150, 12)**********\n"
     ]
    }
   ],
   "source": [
    "# Get data shape, info, columns, & dimensions\n",
    "print (\"*\"*45)\n",
    "print('********** train shape: ' + str(train.shape) + '*'*10)\n",
    "print (train.info())\n",
    "print (\"*\"*43)\n",
    "print('********** test shape: ' + str(test.shape) + '*'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data early on using the train_test_split() function with 75% of the data going into the training set and 25% of the data going into the test set. Then we combined the y and X train data together so we have the full train population.\n",
    "\n",
    "Now, we will not touch the test data for any reason unless we are scaling or creating new features. Only scale and create features based on the training set.\n",
    "\n",
    "### Some methods to combine/add dataframe data\n",
    "If we were combining two dataframes (appending rows), we could use a concat() method:\n",
    "- pd.concat([dataframe_1, dataframe_2], ignore_index=True)\n",
    "\n",
    "If we were combining two dataframes (appending columns), we could use a merge() method:\n",
    "- pd.merge(dataframe_1, dataframe_2, left_on='id', right_on='id')\n",
    "- pd.merge(dataframe_1, dataframe_2, on='id', how='left')\n",
    "\n",
    "If we wanted to add an additional row of data, we could use an append() method:\n",
    "- new_row_data = pd.Series([1,2,3,4], index=['id', 'col1', col2'...]]\n",
    "- dataframe_1.append(new_row_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null values\n",
    "\n",
    "Are there any null values? Let's show column names with any null data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 columns with missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null Data Count</th>\n",
       "      <th>Null Data Pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>203</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher</th>\n",
       "      <td>44</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Null Data Count  Null Data Pct\n",
       "Year                   203           1.63\n",
       "Publisher               44           0.35"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get nulls\n",
    "null_cols = pd.DataFrame(train.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\n",
    "null_cols_pct = pd.DataFrame(round(train.isnull().sum().sort_values(ascending=False)*100/len(train), 2), columns=['Null Data Pct'])\n",
    "\n",
    "# Combine the two dataframes (axis=1) with column names using the keys parameter\n",
    "null_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n",
    "\n",
    "# Get null features only\n",
    "all_nulls = null_cols_df[null_cols_df['Null Data Pct']>0]\n",
    "\n",
    "# Print\n",
    "print('There are', len(all_nulls), 'columns with missing values.')\n",
    "all_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
