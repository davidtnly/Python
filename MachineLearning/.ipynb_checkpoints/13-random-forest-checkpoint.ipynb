{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is a type of bootstrap algorithm (bagging) and a type of ensemble machine learning algorithm that can be used for both classification and regression problems.\n",
    "\n",
    "#### Quick Background on Bootstrap\n",
    "First, to understand how a Random Forest works, we will start with the bootstrap method. Bootstrapping is a powerful statistical method for estimating a quantity from a data sample. This is easiest to understand if the quantity is a descriptive statistic such as a mean or a standard deviation.\n",
    "\n",
    "If our sample is small (not having enough training samples), we can improve the estimate of our mean by using the bootstrap procedure. We will create many sub-samples from the same dataset that is randomized and calculate the mean of each sub-sample. Calculate the average of all of our collected means and use that as our estimated mean for the data. This let's us move onto bootstrap aggregation.\n",
    "\n",
    "#### Quick Background on Bootstrap Aggregation\n",
    "Bootstrap Aggregation is a general procedure that can be used to reduce the variance for those algorithm that have high variance (overfitting). An algorithm that has high variance are decision trees, like classification and regression trees (CART).\n",
    "\n",
    "The main idea is that we are averaging multiple sets of observations to reduce variance. \n",
    "\n",
    "- Take many training sets from the population\n",
    "- Build a separate model using each set and take an average of all of the results\n",
    "    \n",
    "#### Quick Background on Ensemble Method\n",
    "An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model. So think of multiple weak learning models creating a stronger model when it is combined with an averaged result.\n",
    "\n",
    "#### Main Difference Between Bootstrapping & Bagging\n",
    "- Bootstrapping: sampling technique (draws repeated samples)\n",
    "- Bagging: machine learning ensemble technique based on bootstrap samples\n",
    "\n",
    "#### Decision Trees & Random Forest\n",
    "A problem with decision trees like CART is that they are greedy. They choose which variable to split on using a greedy algorithm that minimizes error. \n",
    "\n",
    "Greedy meaning that the algorithm makes the optimal choice at each step as it attempts to find the overall optimal way to solve the entire problem. This means that it makes decisions based only on the information it has at any one step, without regard to the overall problem. Random Forest fixes that with a method called pruning. The decision to split at each node is made according to the metric called purity. A node is 100% impure when a node is split evenly 50/50 and 100% pure when all of its data belongs to a single class. This specific subject can be discussed in a separate notebook about decision trees.\n",
    "\n",
    "### Versatility\n",
    "\n",
    "#### Estimated OOB Performance\n",
    "\"For each bootstrap sample taken from the training data, there will be samples left behind that were not included. These samples are called Out-Of-Bag samples or OOB.\n",
    "\n",
    "The performance of each model on its left out samples when averaged can provide an estimated accuracy of the bagged models. This estimated performance is often called the OOB estimate of performance.\n",
    "\n",
    "These performance measures are reliable test error estimate and correlate well with cross validation estimates.\"\n",
    "\n",
    "#### Variable Importance\n",
    "It doesn't just have a CV method within the algorithm, but we can calculate how much the error function drops for a variable at each split point making the algorithm a very popular one to use.\n",
    "\n",
    "In regression problems this may be the drop in sum squared error and in classification this might be the Gini score.\n",
    "\n",
    "These drops in error can be averaged across all decision trees and output to provide an estimate of the importance of each input variable. The greater the drop when the variable was chosen, the greater the importance.\n",
    "\n",
    "These outputs can help identify subsets of input variables that may be most or least relevant to the problem and suggest at possible feature selection experiments you could perform where some features are removed from the dataset.\n",
    "\n",
    "    Many of the notes about OOB/VarImp from MLM.\n",
    "    \n",
    "#### Advantages\n",
    "- Solves both classification and regression\n",
    "- Handles large data sets with higher dimensionality. \n",
    "- Handles thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs variable importance\n",
    "- Has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing (bagging method)\n",
    "- Handles imbalanced classes better than most algorithms\n",
    "- Has in-model cross-validation method (OOB testing)\n",
    "\n",
    "#### Disadvantages\n",
    "- Not the best explainable algorithm due to the randomness of feature selections per tree\n",
    "    - Control on what goes in the model is difficult\n",
    "- Memory usage is higher than most algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "1. Title: Facebook performance metrics\n",
    "2. Sources\n",
    "   Created by: SÃ©rgio Moro, Paulo Rita and Bernardo Vala (ISCTE-IUL) @ 2016\n",
    "3. Past Usage:\n",
    "    - The full dataset was described and analyzed in:\n",
    "    - S. Moro, P. Rita and B. Vala. Predicting social media performance metrics and evaluation of the impact on \n",
    "   brand building: A data mining approach. Journal of Business Research, Elsevier, In press, Available online \n",
    "   since 28 February 2016.\n",
    "4. Relevant Information:\n",
    "    - The data is related to posts' published during the year of 2014 on the Facebook's page of a renowned cosmetics brand.\n",
    "    - This dataset contains 500 of the 790 rows and part of the features analyzed by Moro et al. (2016). The remaining were omitted due to confidentiality issues.\n",
    "5. Number of Instances: 500\n",
    "6. Number of Attributes: 19\n",
    "7. Missing Attribute Values: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/13-facebook-likes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new function\n",
    "def get_nulls(df):\n",
    "    \n",
    "    # Get null pct and counts\n",
    "    null_cols = pd.DataFrame(df.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\n",
    "    null_cols_pct = pd.DataFrame(round(df.isnull().sum().sort_values(ascending=False)/len(df),2), columns=['Null Data Pct'])\n",
    "\n",
    "    # Combine dataframes horizontally\n",
    "    null_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n",
    "\n",
    "    all_nulls = null_cols_df[null_cols_df['Null Data Pct']>0]\n",
    "\n",
    "    # Print\n",
    "    print('There are', len(all_nulls), 'columns with missing values.')\n",
    "    return all_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 columns with missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null Data Count</th>\n",
       "      <th>Null Data Pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>share</th>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Null Data Count  Null Data Pct\n",
       "share                4           0.01"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nulls(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.share = data.share.fillna(data.share.median())\n",
    "data.like = data.like.fillna(data.like.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns with missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null Data Count</th>\n",
       "      <th>Null Data Pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Null Data Count, Null Data Pct]\n",
       "Index: []"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nulls(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies\n",
    "data = pd.get_dummies(data, drop_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X / y Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training\n",
    "y = data['Total Interactions']\n",
    "X = data.drop(['Total Interactions'], axis=1)\n",
    "\n",
    "# # Re-test with top 6 features\n",
    "# high_varimp = ['like', 'share', 'Lifetime Post reach by people who like your Page', 'comment',\n",
    "#                'Lifetime Post Total Reach', 'Lifetime Engaged Users']\n",
    "# X = data[high_varimp]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import split module\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Page total likes\n",
      "1 Lifetime Post Total Reach\n",
      "2 Lifetime Post Total Impressions\n",
      "3 Lifetime Engaged Users\n",
      "4 Lifetime Post Consumers\n",
      "5 Lifetime Post Consumptions\n",
      "6 Lifetime Post Impressions by people who have liked your Page\n",
      "7 Lifetime Post reach by people who like your Page\n",
      "8 Lifetime People who have liked your Page and engaged with your post\n",
      "9 comment\n",
      "10 like\n",
      "11 share\n"
     ]
    }
   ],
   "source": [
    "for idx, val in enumerate(X_train):\n",
    "    print(idx, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.reset_index(inplace=True, drop=True)\n",
    "# X_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Transform the variables to be on the same scale\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will calculate error\n",
    "def base_rmse(y, y_pred):\n",
    "    '''\n",
    "    Return the sqrt of the mean squared error between the observed and predicted values\n",
    "    '''\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix NA Issue (replace np array values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check NA\n",
    "np.where(np.isnan(X_train))\n",
    "X_train = np.nan_to_num(X_train)\n",
    "np.where(np.isnan(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE: 54.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# Random Forest\n",
    "model = RandomForestRegressor(n_estimators=500, random_state=10)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "rf_rmse = base_rmse(y_test, y_pred)\n",
    "print('Random Forest RMSE: {:.2f}'.format(rf_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = X.columns\n",
    "\n",
    "# Create a new function to capture feature importance for free models (RF, GB, XGB)\n",
    "def feature_importance(model):\n",
    "    \n",
    "    importance = pd.DataFrame({'Feature': headers,\n",
    "                               'Importance': np.round(model.feature_importances_,5)})\n",
    "    \n",
    "    importance = importance.sort_values(by='Importance', ascending=False).set_index('Feature')\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.62148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share</th>\n",
       "      <td>0.09315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lifetime Post reach by people who like your Page</th>\n",
       "      <td>0.08174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment</th>\n",
       "      <td>0.08097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <td>0.06536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lifetime Engaged Users</th>\n",
       "      <td>0.03358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lifetime People who have liked your Page and engaged with your post</th>\n",
       "      <td>0.00996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
       "      <td>0.00466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lifetime Post Total Impressions</th>\n",
       "      <td>0.00454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lifetime Post Consumers</th>\n",
       "      <td>0.00289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Page total likes</th>\n",
       "      <td>0.00098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lifetime Post Consumptions</th>\n",
       "      <td>0.00069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Importance\n",
       "Feature                                                       \n",
       "like                                                   0.62148\n",
       "share                                                  0.09315\n",
       "Lifetime Post reach by people who like your Page       0.08174\n",
       "comment                                                0.08097\n",
       "Lifetime Post Total Reach                              0.06536\n",
       "Lifetime Engaged Users                                 0.03358\n",
       "Lifetime People who have liked your Page and en...     0.00996\n",
       "Lifetime Post Impressions by people who have li...     0.00466\n",
       "Lifetime Post Total Impressions                        0.00454\n",
       "Lifetime Post Consumers                                0.00289\n",
       "Page total likes                                       0.00098\n",
       "Lifetime Post Consumptions                             0.00069"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
